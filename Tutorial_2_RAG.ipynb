{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Squigspear/xCyberLLM/blob/main/Tutorial_2_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q7F1hyhQH23"
      },
      "source": [
        "This tutorial showcases **Retrieval Augmentation Generation** with a LLM, where a document (CCOP) is referred to when the model is queried.\n",
        "\n",
        "It can be ran using either:\n",
        "\n",
        "  >Open-sourced LLAMA 2 7B 5 bit quantized model *or*\n",
        "\n",
        "  >Microsoft Azure OpenAI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEwthCd1PHWe"
      },
      "source": [
        "# Running LLAMA 2 on Colab - No UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_p7RvpFIVJ-"
      },
      "source": [
        "## Installing of required packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY3mUTcBIi1Q"
      },
      "source": [
        "Downloading quantized model from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.0.310 --quiet\n",
        "!export LLAMA_CUBLAS=1\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python==0.2.11 --quiet\n",
        "!pip install sentence-transformers==2.2.2 --quiet\n",
        "!pip install chromadb==0.3.26 --quiet\n",
        "!pip install ctransformers --quiet\n",
        "!pip install pypdf --quiet"
      ],
      "metadata": {
        "id": "LWSTOKfg_r-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_K_M.gguf"
      ],
      "metadata": {
        "id": "Rwr8QpAlALcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBr3qEHaIoSN"
      },
      "source": [
        "## Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    n_ctx = 4096,\n",
        "    model_path=\"/content/llama-2-7b-chat.Q5_K_M.gguf\", # location of the model, llama-2-13b-chat.Q4_0.gguf\n",
        "    temperature=0.2,                 # temperature\n",
        "    max_tokens=2000,                 # Max. number of tokens to be generated\n",
        "    top_p=0.9,                    # top_p = 0.9\n",
        "    top_k=30,                     # top_k = 30\n",
        "    n_gpu_layers=200,                 # number of layers to offload to GPU\n",
        "    verbose=True, # Verbose is required to pass to the callback manager\n",
        "    callback_manager=callback_manager,\n",
        "    n_batch=200,          # number of token generation in parallel\n",
        ")"
      ],
      "metadata": {
        "id": "eM5Xq0yxCOV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb-7w-0KbPhm"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.indexes.vectorstore import VectorstoreIndexCreator\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import AzureOpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from chromadb.config import Settings\n",
        "\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<>\\n\", \"\\n<>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"You are a helpful, respectful and honest assistant.\"\n",
        "\n",
        "def create_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT):\n",
        "  SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
        "  prompt = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "  return \"\" + prompt"
      ],
      "metadata": {
        "id": "-hrZsjhlIPOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Only return the helpful concise answer below and nothing else.\n",
        "Helpful answer:\"\"\""
      ],
      "metadata": {
        "id": "j3cYtVl7Ikwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = create_prompt(qa_template)\n",
        "print(prompt_template)\n",
        "prompt = PromptTemplate(template=prompt_template,input_variables=['context', 'question'])"
      ],
      "metadata": {
        "id": "s6AFMbg_IeYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2h9Boz-PW89"
      },
      "source": [
        "## Loading DocumentQA using Langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = \"./\"\n",
        "\n",
        "CHROMA_SETTINGS = Settings(\n",
        "    chroma_db_impl = 'duckdb+parquet',  # use duckdb to r/w parquet file\n",
        "    anonymized_telemetry = False     # refuse telemetry of usage info\n",
        ")\n",
        "\n",
        "model_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
        "model_kwargs = {'device': 'cuda'}      # you must have a gpu, otherwise change it to cpu\n",
        "encode_kwargs = {'normalize_embeddings': True}\n",
        "embedding_func = HuggingFaceEmbeddings(\n",
        "    model_name = model_name,\n",
        "    model_kwargs = model_kwargs,\n",
        "    encode_kwargs = encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "SgR2YcYQMpMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U58a_U7KdWCh"
      },
      "outputs": [],
      "source": [
        "def setup_new_qa(filename):\n",
        "  global llm\n",
        "  # Loading of documents\n",
        "  loader = PyPDFLoader(filename)\n",
        "  documents = loader.load()\n",
        "  # Splitting of text into chunks\n",
        "\n",
        "  text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
        "  texts = text_splitter.split_documents(documents)\n",
        "\n",
        "  # Embeddings to use, to transform the document\n",
        "  # embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2',\n",
        "  #                                   model_kwargs={'device':'cpu'})\n",
        "\n",
        "  text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
        "  chunked_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "  # configuration of the vector store\n",
        "\n",
        "  print(\"Creating new vector store...\")\n",
        "  db = Chroma.from_documents(\n",
        "      chunked_docs,\n",
        "      embedding_func,\n",
        "      persist_directory=persist_directory,\n",
        "      client_settings=CHROMA_SETTINGS\n",
        "  )\n",
        "\n",
        "  db.persist()\n",
        "\n",
        "  retriever = db.as_retriever(search_type = 'similarity', search_kwargs = {'k':4}) # with top k ranked matches\n",
        "\n",
        "  qa = ConversationalRetrievalChain.from_llm(llm, retriever,verbose = True, return_source_documents=True, combine_docs_chain_kwargs={\"prompt\": prompt})\n",
        "\n",
        "  return qa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lTytpW4cy8p"
      },
      "source": [
        "## Gradio Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJUJLWQ92g6R"
      },
      "outputs": [],
      "source": [
        "!pip install gradio==3.48.0 pydantic --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UI for Document Q&A"
      ],
      "metadata": {
        "id": "nr1WIYvXEovC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae74757b",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import random\n",
        "import time\n",
        "\n",
        "COUNT = 0\n",
        "chat_history = []\n",
        "qa = None\n",
        "\n",
        "def process_file(files):\n",
        "    global qa\n",
        "    file_paths = [file.name for file in files]\n",
        "    qa = setup_new_qa(file_paths[0]) # btn.name\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox()\n",
        "    file_output = gr.File()\n",
        "    btn = gr.UploadButton(\"Click to Upload a File\", file_types=[\"PDF\"], file_count=\"multiple\")\n",
        "    btn.upload(process_file, btn, file_output)\n",
        "\n",
        "    used_letters_var = gr.State([])\n",
        "\n",
        "    def respond(message, chat_history, btn):\n",
        "        global COUNT, query, qa\n",
        "\n",
        "        #'''\n",
        "        #if not btn:\n",
        "        #    raise gr.Error(message = 'Document Not Found.')\n",
        "\n",
        "        #if COUNT == 0:\n",
        "        #  #filename = \"./datafile/ccop/CCOP.pdf\"\n",
        "        #  filename = btn.name\n",
        "        #  qa = setup_new_qa(filename) # btn.name\n",
        "        #  COUNT += 1\n",
        "\n",
        "\n",
        "        req_msg = str(message)\n",
        "        res_msg = ''\n",
        "        try:\n",
        "          if qa == None:\n",
        "            return \"\", chat_history\n",
        "          result = qa({'question': req_msg, 'chat_history': []})\n",
        "          if result['answer'] == '':\n",
        "            res_msg = 'No answer'\n",
        "            return\n",
        "          else:\n",
        "            res_msg =  str(result['answer'][:1000])\n",
        "        except:\n",
        "          raise gr.Error('LLM hang!')\n",
        "\n",
        "        bot_message = res_msg\n",
        "\n",
        "        #print(message)\n",
        "        #bot_message = message\n",
        "        #chat_history = []\n",
        "        chat_history.append((message, bot_message))\n",
        "        #time.sleep(2)\n",
        "        return \"\", chat_history\n",
        "\n",
        "#     btn.upload(inputs = [btn])\n",
        "    msg.submit(respond, [msg, chatbot, btn], [msg, chatbot])\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using AzureOpenAI as LLM"
      ],
      "metadata": {
        "id": "eliAAED30knO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai --quiet\n",
        "!pip install tiktoken --quiet"
      ],
      "metadata": {
        "id": "m-xbgjr27FTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "# Setting up API access\n",
        "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://dto-testing.openai.azure.com/\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"ce8e5b0d756f4d83b4dfbbc4ccd08fec\"\n",
        "\n",
        "\n",
        "llm_OAI = AzureOpenAI( temperature = 0.1,\n",
        "                   deployment_name=\"DTO_demo\",  # This is the deployed GPT3 from Azure\n",
        "                   model_name=\"text-davinci-003\",\n",
        "                            )"
      ],
      "metadata": {
        "id": "1KFrnHss0qD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Only return the helpful concise answer below and nothing else.\n",
        "Helpful answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=qa_template,input_variables=['context', 'question'])"
      ],
      "metadata": {
        "id": "byDX8IxX6ThJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_new_qa_OAI(filename):\n",
        "\n",
        "  global llm_OAI\n",
        "  # Loading of documents\n",
        "  loader = PyPDFLoader(filename)\n",
        "  documents = loader.load()\n",
        "  # Splitting of text into chunks\n",
        "\n",
        "  # Splitting of text into chunks\n",
        "\n",
        "  text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
        "  texts = text_splitter.split_documents(documents)\n",
        "\n",
        "  embeddings = OpenAIEmbeddings(deployment=\"DTO_embed\"\n",
        "                                ,model='text-embedding-ada-002'\n",
        "                                ,chunk_size=1) # chunk_size number is peculiarity of Azure OpenAI\n",
        "\n",
        "  db = Chroma.from_documents(texts, embeddings) # vector database\n",
        "\n",
        "  retriever = db.as_retriever(search_type = 'similarity', search_kwargs = {'k':4}) # with top k ranked matches\n",
        "\n",
        "  qa = ConversationalRetrievalChain.from_llm(llm_OAI, retriever,verbose = False, return_source_documents=True, combine_docs_chain_kwargs={\"prompt\": prompt})\n",
        "\n",
        "  return qa"
      ],
      "metadata": {
        "id": "gbZPUw7o60LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import random\n",
        "import time\n",
        "\n",
        "COUNT = 0\n",
        "chat_history = []\n",
        "qa = None\n",
        "\n",
        "def process_file(files):\n",
        "    global qa\n",
        "    file_paths = [file.name for file in files]\n",
        "    qa = setup_new_qa(file_paths[0]) # btn.name\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox()\n",
        "    file_output = gr.File()\n",
        "    btn = gr.UploadButton(\"Click to Upload a File\", file_types=[\"PDF\"], file_count=\"multiple\")\n",
        "    btn.upload(process_file, btn, file_output)\n",
        "\n",
        "    used_letters_var = gr.State([])\n",
        "\n",
        "    def respond(message, chat_history, btn):\n",
        "        global COUNT, query, qa\n",
        "\n",
        "        req_msg = str(message)\n",
        "        res_msg = ''\n",
        "        try:\n",
        "          if qa == None:\n",
        "            return \"\", chat_history\n",
        "          result = qa({'question': req_msg, 'chat_history': []})\n",
        "          if result['answer'] == '':\n",
        "            res_msg = 'No answer'\n",
        "            return\n",
        "          else:\n",
        "            res_msg =  str(result['answer'][:1000])\n",
        "        except:\n",
        "          raise gr.Error('LLM hang!')\n",
        "\n",
        "        bot_message = res_msg\n",
        "        chat_history.append((message, bot_message))\n",
        "        return \"\", chat_history\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot, btn], [msg, chatbot])\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "kiaXuyp37SJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h5rFtpJt7dJ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "J_p7RvpFIVJ-",
        "QBr3qEHaIoSN",
        "V2h9Boz-PW89",
        "4lTytpW4cy8p"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}