{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Squigspear/xCyberLLM/blob/main/Tutorial2/Tutorial_2_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q7F1hyhQH23"
      },
      "source": [
        "This tutorial showcases **Retrieval Augmentation Generation** with a LLM, where a document (CCOP) is referred to when the model is queried. Split into 2 parts, (1) Open-sourced LLAMA 2 7B 5 bit quantized mode (2) Microsoft Azure OpenAI.\n",
        "\n",
        "\n",
        "**Before starting:**\n",
        "\n",
        "1. **Click on Runtime (top) > Change Runtime type -> Python 3, T4 GPU**\n",
        "\n",
        "2. **Click Connect (top right corner)**\n",
        "\n",
        "**To Run**\n",
        "\n",
        "**1. Click on play button (left) or Ctrl-Enter of each cell to run cell**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO-Tvvn9xZUx"
      },
      "source": [
        "## Initialise notebook\n",
        "Runtime: ~ 7mins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrwgNSoJjaBC"
      },
      "outputs": [],
      "source": [
        "#%cd /content/drive/MyDrive/Colab Notebooks/Hackathon\n",
        "\n",
        "# Download from google drive\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "#some file id/ list of file ids parsed from file urls.\n",
        "google_fid_id = '1PFXHKUfiDupWV_K_MLmSWBxajTPvj_B4' #'1jAluS8pfwn8_yp-cyED97OdYXl4-vWJ6'\n",
        "destination = 'dir/dir/utils_tutorial_2_RAG.ipynb'\n",
        "#if zip file ad kwarg unzip=true\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id=google_fid_id,dest_path = destination)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/dir/dir\n",
        "%run 'utils_tutorial_2_RAG.ipynb'"
      ],
      "metadata": {
        "id": "Wxq2HZ0bormX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEwthCd1PHWe"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Running LLAMA 2 on Colab\n",
        "(Open-source LLM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo2PFP23kI47"
      },
      "source": [
        "## **Initialise LLM (Llama-2-7b model)**\n",
        "Optional: Change model and parameters as required\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT9M-Ne-jbwI"
      },
      "outputs": [],
      "source": [
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    n_ctx = 4096,\n",
        "    model_path=\"llama-2-7b-chat.Q5_K_M.gguf\", # location of the model, llama-2-13b-chat.Q4_0.gguf\n",
        "    temperature=0.2,                 # temperature\n",
        "    max_tokens=2000,                 # Max. number of tokens to be generated\n",
        "    top_p=0.9,                    # top_p = 0.9\n",
        "    top_k=30,                     # top_k = 30\n",
        "    n_gpu_layers=200,                 # number of layers to offload to GPU\n",
        "    verbose=True, # Verbose is required to pass to the callback manager\n",
        "    callback_manager=callback_manager,\n",
        "    n_batch=200,          # number of token generation in parallel\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMMoQYOckQG4"
      },
      "source": [
        "## **Customise prompt template**\n",
        "To edit: Change qa_template accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnrUVur2kFAw"
      },
      "outputs": [],
      "source": [
        "qa_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Only return the helpful concise answer below and nothing else.\n",
        "Helpful answer:\"\"\"\n",
        "\n",
        "prompt_template = create_prompt(qa_template)\n",
        "print(prompt_template)\n",
        "prompt = PromptTemplate(template=prompt_template,input_variables=['context', 'question'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwd3dMeXkc5q"
      },
      "source": [
        "## Add DocumentQA using langchain for RAG\n",
        "Optional: Change embedding model accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCn2OTyAmJ7H"
      },
      "outputs": [],
      "source": [
        "model_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
        "model_kwargs = {'device': 'cuda'}      # you must have a gpu, otherwise change it to cpu\n",
        "encode_kwargs = {'normalize_embeddings': True}\n",
        "embedding_func = HuggingFaceEmbeddings(\n",
        "    model_name = model_name,\n",
        "    model_kwargs = model_kwargs,\n",
        "    encode_kwargs = encode_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ4ndUc4mn4b"
      },
      "source": [
        "## Launch Gradio UI\n",
        "\n",
        "To do:\n",
        "1. Upload CCOP.pdf\n",
        "2. Type question in chatbox\n",
        "3. To end - click on stop button on top left of cell\n",
        "\n",
        "(Make sure the top left icon of the cell is running)\n",
        "\n",
        "\n",
        "Error: restart the cell by clicking stop -> play)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U7edZLVmanA"
      },
      "outputs": [],
      "source": [
        "COUNT = 0\n",
        "chat_history = []\n",
        "qa = None\n",
        "\n",
        "setup_gradio()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eliAAED30knO"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Using AzureOpenAI as LLM\n",
        "(Closed-source LLM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUn6JOvJBKmY"
      },
      "source": [
        "## **Initialise LLM (GPT3.5 / 4)**\n",
        "Optional: Change model and parameters as required. Select either GPT3.5 or 4\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT3.5"
      ],
      "metadata": {
        "id": "TU_kaxY3gOZG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KFrnHss0qD3"
      },
      "outputs": [],
      "source": [
        "# GPT 3.5\n",
        "# Setting up API access\n",
        "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://dto-testing.openai.azure.com/\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"ce8e5b0d756f4d83b4dfbbc4ccd08fec\"\n",
        "\n",
        "embeddings = AzureOpenAIEmbeddings(deployment=\"DTO_embed\"\n",
        "                              ,model='text-embedding-ada-002'\n",
        "                              ,chunk_size=1) # chunk_size number is peculiarity of Azure OpenAI\n",
        "\n",
        "llm_OAI = AzureOpenAI( temperature = 0.1, #Change the temperature to increase\n",
        "                   deployment_name=\"DTO_demo\",  # This is the deployed GPT3 from Azure\n",
        "                   model_name=\"text-davinci-003\",\n",
        "                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Or GPT4\n"
      ],
      "metadata": {
        "id": "QrROTgZEgSJu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3bPfdxD-wpq"
      },
      "outputs": [],
      "source": [
        "# GPT 4\n",
        "# from langchain.chat_models.azure_openai import AzureChatOpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2023-03-15-preview\"\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://ccm-3.openai.azure.com/\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"6daf5804dd1e4d05a5d1079ac52e40a7\"\n",
        "os.environ[\"AZURE_ENDPOINT\"] = \"https://ccm-3.openai.azure.com/openai\"\n",
        "\n",
        "\n",
        "embeddings = AzureOpenAIEmbeddings(deployment=\"ccm-embedding\"\n",
        "                              ,model='text-embedding-ada-002'\n",
        "                              ,chunk_size=1) # chunk_size number is peculiarity of Azure OpenAI\n",
        "\n",
        "llm_OAI = AzureChatOpenAI( temperature = 0.1,\n",
        "                   deployment_name=\"ccm-gpt4\",  # This is the deployed GPT4 from Azure\n",
        "                   model_name=\"gpt-4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byDX8IxX6ThJ"
      },
      "outputs": [],
      "source": [
        "qa_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Only return the helpful concise answer below and nothing else.\n",
        "Helpful answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=qa_template,input_variables=['context', 'question'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fpm-9suREqkj"
      },
      "source": [
        "## Launch Gradio UI\n",
        "\n",
        "To do:\n",
        "1. Upload CCOP.pdf\n",
        "2. Type question in chatbox\n",
        "3. To end - click on stop button on top left of cell\n",
        "\n",
        "(Make sure the top left icon of the cell is running)\n",
        "\n",
        "\n",
        "Error: restart the cell by clicking stop -> play)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7JKFk40Hzua"
      },
      "outputs": [],
      "source": [
        "COUNT = 0\n",
        "chat_history = []\n",
        "setup_gradio_OAI()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "RO-Tvvn9xZUx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}