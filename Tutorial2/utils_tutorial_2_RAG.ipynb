{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwBo2KZpPLFTdWZ5IP2Ia/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Squigspear/xCyberLLM/blob/main/Tutorial2/utils_tutorial_2_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook to initialise dependencies and functions for hackathon tutorial 2"
      ],
      "metadata": {
        "id": "KnpVCf_WeS7q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HarqByiehvCg"
      },
      "outputs": [],
      "source": [
        "# To format the outputs from jupyter notebook\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  var = pd.DataFrame()\n",
        "except NameError: # Install and import required packagess if required\n",
        "  print('Installing and importing packages')\n",
        "  !wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_K_M.gguf\n",
        "  # https://colab.research.google.com/drive/1PFXHKUfiDupWV_K_MLmSWBxajTPvj_B4?usp=sharing\n",
        "  !pip install langchain==0.0.310 --quiet\n",
        "  !export LLAMA_CUBLAS=1\n",
        "  !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python==0.2.11 --quiet\n",
        "  !pip install sentence-transformers==2.2.2 --quiet\n",
        "  !pip install chromadb==0.3.26 --quiet\n",
        "  !pip install ctransformers --quiet\n",
        "  !pip install pypdf --quiet\n",
        "  !pip install\n",
        "  !pip install gradio==3.48.0 pydantic --quiet\n",
        "\n",
        "  !pip install openai --quiet\n",
        "  !pip install tiktoken --quiet\n",
        "\n",
        "  !pip install pandas\n",
        "  import gradio as gr\n",
        "  import random\n",
        "  import time\n",
        "  from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "  import pandas as pd\n",
        "  from langchain.llms import LlamaCpp\n",
        "  from langchain.callbacks.manager import CallbackManager\n",
        "  from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "  from langchain import PromptTemplate, LLMChain\n",
        "  from langchain.memory import ConversationSummaryMemory\n",
        "  from langchain.chains import ConversationalRetrievalChain\n",
        "  from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "  from langchain.text_splitter import CharacterTextSplitter\n",
        "  from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "  from langchain.vectorstores import FAISS\n",
        "  from langchain.docstore.document import Document\n",
        "  from langchain.indexes.vectorstore import VectorstoreIndexCreator\n",
        "  from langchain.chains import RetrievalQA\n",
        "  from langchain.llms import AzureOpenAI\n",
        "  from langchain.chains.question_answering import load_qa_chain\n",
        "  from langchain.document_loaders import TextLoader\n",
        "  from langchain.document_loaders import PyPDFLoader\n",
        "  from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "  from langchain.embeddings import HuggingFaceEmbeddings\n",
        "  from langchain.vectorstores import Chroma\n",
        "  from chromadb.config import Settings\n",
        "\n",
        "  import pandas as pd\n",
        "  import os\n",
        "\n",
        "  # For LLM\n",
        "  callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n"
      ],
      "metadata": {
        "id": "tPJMO78bhzLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# * DocumentQA using Langchain\n"
      ],
      "metadata": {
        "id": "iMDkcTO2kxiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = \"./\"\n",
        "\n",
        "CHROMA_SETTINGS = Settings(\n",
        "    chroma_db_impl = 'duckdb+parquet',  # use duckdb to r/w parquet file\n",
        "    anonymized_telemetry = False     # refuse telemetry of usage info\n",
        ")\n",
        "\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<>\\n\", \"\\n<>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"You are a helpful, respectful and honest assistant.\"\n",
        "\n",
        "def create_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT):\n",
        "  SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
        "  prompt = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "  return \"\" + prompt\n",
        "\n",
        "def setup_new_qa(filename):\n",
        "  global llm\n",
        "  # Loading of documents\n",
        "  loader = PyPDFLoader(filename)\n",
        "  documents = loader.load()\n",
        "  # Splitting of text into chunks\n",
        "\n",
        "  text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
        "  texts = text_splitter.split_documents(documents)\n",
        "\n",
        "  # Embeddings to use, to transform the document\n",
        "  # embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2',\n",
        "  #                                   model_kwargs={'device':'cpu'})\n",
        "\n",
        "  text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
        "  chunked_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "  # configuration of the vector store\n",
        "\n",
        "  print(\"Creating new vector store...\")\n",
        "  db = Chroma.from_documents(\n",
        "      chunked_docs,\n",
        "      embedding_func,\n",
        "      persist_directory=persist_directory,\n",
        "      client_settings=CHROMA_SETTINGS\n",
        "  )\n",
        "\n",
        "  db.persist()\n",
        "\n",
        "  retriever = db.as_retriever(search_type = 'similarity', search_kwargs = {'k':4}) # with top k ranked matches\n",
        "\n",
        "  qa = ConversationalRetrievalChain.from_llm(llm, retriever,verbose = True, return_source_documents=True, combine_docs_chain_kwargs={\"prompt\": prompt})\n",
        "\n",
        "  return qa\n",
        "\n",
        "persist_directory = \"./\"\n",
        "\n",
        "CHROMA_SETTINGS = Settings(\n",
        "    chroma_db_impl = 'duckdb+parquet',  # use duckdb to r/w parquet file\n",
        "    anonymized_telemetry = False     # refuse telemetry of usage info\n",
        ")\n"
      ],
      "metadata": {
        "id": "5uQ3uT7lk1w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For OpenAI RAG"
      ],
      "metadata": {
        "id": "gH4fWFe19RoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_new_qa_OAI(filename):\n",
        "  global llm_OAI\n",
        "  # Loading of documents\n",
        "  loader = PyPDFLoader(filename)\n",
        "  documents = loader.load()\n",
        "\n",
        "  # Splitting of text into chunks\n",
        "  text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
        "  texts = text_splitter.split_documents(documents)\n",
        "\n",
        "  embeddings = OpenAIEmbeddings(deployment=\"DTO_embed\"\n",
        "                                ,model='text-embedding-ada-002'\n",
        "                                ,chunk_size=1) # chunk_size number is peculiarity of Azure OpenAI\n",
        "\n",
        "  db = Chroma.from_documents(texts, embeddings) # vector database\n",
        "\n",
        "  retriever = db.as_retriever(search_type = 'similarity', search_kwargs = {'k':4}) # with top k ranked matches\n",
        "\n",
        "  qa = ConversationalRetrievalChain.from_llm(llm_OAI, retriever,verbose = False, return_source_documents=True, combine_docs_chain_kwargs={\"prompt\": prompt})\n",
        "\n",
        "  return qa"
      ],
      "metadata": {
        "id": "xcJKHVhz9UgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For GRADIO UI\n",
        "UI for Document Q&A"
      ],
      "metadata": {
        "id": "JFu8_O3SiQvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Llama2\n",
        "def process_file(files):\n",
        "    global qa\n",
        "    file_paths = [file.name for file in files]\n",
        "    qa = setup_new_qa(file_paths[0]) # btn.name\n",
        "\n",
        "def setup_gradio():\n",
        "  with gr.Blocks() as demo:\n",
        "      chatbot = gr.Chatbot()\n",
        "      msg = gr.Textbox()\n",
        "      file_output = gr.File()\n",
        "      btn = gr.UploadButton(\"Click to Upload a File\", file_types=[\"PDF\"], file_count=\"multiple\")\n",
        "      btn.upload(process_file, btn, file_output)\n",
        "\n",
        "      used_letters_var = gr.State([])\n",
        "\n",
        "      def respond(message, chat_history, btn):\n",
        "          global COUNT, query, qa\n",
        "\n",
        "          #'''\n",
        "          #if not btn:\n",
        "          #    raise gr.Error(message = 'Document Not Found.')\n",
        "\n",
        "          #if COUNT == 0:\n",
        "          #  #filename = \"./datafile/ccop/CCOP.pdf\"\n",
        "          #  filename = btn.name\n",
        "          #  qa = setup_new_qa(filename) # btn.name\n",
        "          #  COUNT += 1\n",
        "\n",
        "          req_msg = str(message)\n",
        "          res_msg = ''\n",
        "          try:\n",
        "            if qa == None:\n",
        "              return \"\", chat_history\n",
        "            result = qa({'question': req_msg, 'chat_history': []})\n",
        "            if result['answer'] == '':\n",
        "              res_msg = 'No answer'\n",
        "              return\n",
        "            else:\n",
        "              res_msg =  str(result['answer'][:1000])\n",
        "          except:\n",
        "            raise gr.Error('LLM hang!')\n",
        "\n",
        "          bot_message = res_msg\n",
        "\n",
        "          #print(message)\n",
        "          #bot_message = message\n",
        "          #chat_history = []\n",
        "          chat_history.append((message, bot_message))\n",
        "          #time.sleep(2)\n",
        "          return \"\", chat_history\n",
        "\n",
        "  #     btn.upload(inputs = [btn])\n",
        "      msg.submit(respond, [msg, chatbot, btn], [msg, chatbot])\n",
        "  demo.launch(debug=True)\n",
        "\n",
        "# OAI\n",
        "def process_file_OAI(files):\n",
        "    global qa\n",
        "    file_paths = [file.name for file in files]\n",
        "    qa = setup_new_qa_OAI(file_paths[0]) # btn.name\n",
        "\n",
        "\n",
        "def setup_gradio_OAI():\n",
        "  with gr.Blocks() as demo_OAI:\n",
        "      chatbot = gr.Chatbot()\n",
        "      msg = gr.Textbox()\n",
        "      file_output = gr.File()\n",
        "      btn = gr.UploadButton(\"Click to Upload a File\", file_types=[\"PDF\"], file_count=\"multiple\")\n",
        "      btn.upload(process_file_OAI, btn, file_output)\n",
        "\n",
        "      used_letters_var = gr.State([])\n",
        "\n",
        "      def respond(message, chat_history, btn):\n",
        "          global COUNT, query, qa\n",
        "\n",
        "          #'''\n",
        "          #if not btn:\n",
        "          #    raise gr.Error(message = 'Document Not Found.')\n",
        "\n",
        "          #if COUNT == 0:\n",
        "          #  #filename = \"./datafile/ccop/CCOP.pdf\"\n",
        "          #  filename = btn.name\n",
        "          #  qa = setup_new_qa(filename) # btn.name\n",
        "          #  COUNT += 1\n",
        "\n",
        "          req_msg = str(message)\n",
        "          res_msg = ''\n",
        "          try:\n",
        "            if qa == None:\n",
        "              return \"\", chat_history\n",
        "            result = qa({'question': req_msg, 'chat_history': []})\n",
        "            if result['answer'] == '':\n",
        "              res_msg = 'No answer'\n",
        "              return\n",
        "            else:\n",
        "              res_msg =  str(result['answer'][:1000])\n",
        "          except:\n",
        "            raise gr.Error('LLM hang!')\n",
        "\n",
        "          bot_message = res_msg\n",
        "\n",
        "          #print(message)\n",
        "          #bot_message = message\n",
        "          #chat_history = []\n",
        "          chat_history.append((message, bot_message))\n",
        "          #time.sleep(2)\n",
        "          return \"\", chat_history\n",
        "\n",
        "  #     btn.upload(inputs = [btn])\n",
        "      msg.submit(respond, [msg, chatbot, btn], [msg, chatbot])\n",
        "  demo_OAI.launch(debug=True)"
      ],
      "metadata": {
        "id": "YAj-rMI5iX79"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}